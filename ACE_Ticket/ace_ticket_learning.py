"""
ACEå¤šè½®é‚®ä»¶å­¦ä¹ æµ‹è¯•è„šæœ¬

æ•°æ®å¤„ç†é€»è¾‘ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼š
1. ä»æ•°æ®åº“æŸ¥è¯¢é‚®ä»¶ä¼šè¯ï¼ˆæ¯ä¸ªä¼šè¯åŒ…å«å¤šå°é‚®ä»¶ï¼‰
2. å¯¹æ¯ä¸ªä¼šè¯ï¼š
   - ground_truth = æœ€åä¸€å°senté‚®ä»¶ï¼ˆä¸“å®¶å›å¤ï¼Œä½œä¸ºå­¦ä¹ ç›®æ ‡ï¼‰
   - history = é™¤ground_truthå¤–çš„æ‰€æœ‰é‚®ä»¶ï¼ˆä½œä¸ºè®­ç»ƒè¾“å…¥contextï¼‰
   - topic = ä»æ‰€æœ‰é‚®ä»¶ä¸­æå–ï¼ˆåŒ…æ‹¬ground_truthï¼Œç”¨äºç”Ÿæˆquestionï¼‰
   - workflow = åŸºäºæ‰€æœ‰é‚®ä»¶æå–ï¼ˆåŒ…æ‹¬ground_truthï¼‰
3. è®­ç»ƒæ ·æœ¬æ„é€ ï¼š
   - question = åŸºäºtopicç”Ÿæˆçš„é—®é¢˜
   - context = {"workflow_result": workflow, "history": history}  # ä¸åŒ…å«ground_truth
   - ground_truth = é¢„å¤„ç†åçš„ä¸“å®¶å›å¤

Ticketç³»ç»Ÿæ”¯æŒï¼š
1. ä» ticket API æ‹‰å– ticket åˆ—è¡¨å’Œæ¶ˆæ¯è¯¦æƒ…
2. é€šè¿‡ replyMessageId æ„å»ºå®Œæ•´çš„å¯¹è¯å†å²
3. è¯†åˆ« CSR çš„å›å¤ä½œä¸º ground_truthï¼Œæˆ–æ ‡è®°ä¸º resolved
"""

import asyncio
import json
import logging
import os
import sys
import re
from pathlib import Path
from dotenv import load_dotenv
import httpx
import pymysql
from pymysql.cursors import DictCursor
from sqlalchemy import create_engine, text  # ç”¨äºè¿æ¥PostgreSQL
from typing import List, Dict, Optional, Tuple
from html import unescape

# åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆä»é¡¹ç›®æ ¹ç›®å½•åŠ è½½ .env æ–‡ä»¶ï¼‰
project_root = Path(__file__).parent.parent
load_dotenv(project_root / '.env')

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(project_root))

from ace import LiteLLMClient, Generator, Reflector, Curator, Playbook, OfflineAdapter, Sample, TaskEnvironment, EnvironmentResult
from email_evaluation_agent import EmailEvaluationAgent
from ace_eval_agent import ACEEvaluationAgent

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å¯¼å…¥ HTML è½¬ Markdown è½¬æ¢å™¨
from html_to_md import HtmlToMarkdownConverter

# Ticketç³»ç»Ÿé…ç½®ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
TICKET_API_BASE_URL = os.getenv('TICKET_API_BASE_URL', 'https://unisticket.item.com/api/item-tickets/v1/iam')
TICKET_API_KEY = os.getenv('TICKET_API_KEY', '5a20d885-455d-4801-9e31-1e9196c13367')
DEFAULT_STAFF_ID = os.getenv('TICKET_STAFF_ID', '91')  # Celine Escorido
DEFAULT_STAFF_EMAIL = os.getenv('TICKET_STAFF_EMAIL', 'cs@unisco.com')
DEFAULT_STAFF_NAME = os.getenv('TICKET_STAFF_NAME', 'Celine Escorido')
DEFAULT_STAFF_ROLE = os.getenv('TICKET_STAFF_ROLE', 'CSR')  # Customer Service Representative
MAX_TICKETS = int(os.getenv('MAX_TICKETS', '200'))  # æœ€å¤šä½¿ç”¨æœ€è¿‘çš„200æ¡
BATCH_SIZE = int(os.getenv('TICKET_BATCH_SIZE', '20'))  # æ¯æ‰¹å¤„ç†çš„ticketæ•°é‡ï¼Œç”¨äºèŠ‚çœå†…å­˜


def get_source_db_connection():
    """è·å–æºæ•°æ®åº“è¿æ¥ (MySQL - ç”¨äºè¯»å–é‚®ä»¶)"""
    db_config = {
        'host': os.getenv('DEV_DB_HOST'),
        'port': int(os.getenv('DEV_DB_PORT', 3306)),
        'user': os.getenv('DEV_DB_USERNAME'),
        'password': os.getenv('DEV_DB_PASSWORD'),
        'database': os.getenv('DEV_DB_DATABASE'),
        'charset': 'utf8mb4',
        'cursorclass': DictCursor
    }
    
    logger.info(f"è¿æ¥æºæ•°æ®åº“(MySQL): {db_config['host']}:{db_config['port']}/{db_config['database']}")
    
    try:
        connection = pymysql.connect(**db_config)
        logger.info("âœ“ æºæ•°æ®åº“è¿æ¥æˆåŠŸ")
        return connection
    except Exception as e:
        logger.error(f"âœ— æºæ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
        raise


# å…¨å±€å˜é‡ï¼šå¤ç”¨æ•°æ®åº“å¼•æ“ï¼ˆé¿å…é‡å¤åˆ›å»ºï¼‰
_target_db_engine = None

def get_target_db_engine():
    """è·å–ç›®æ ‡æ•°æ®åº“å¼•æ“ (PostgreSQL - ç”¨äºå†™å…¥å­¦ä¹ ç»“æœ)"""
    global _target_db_engine
    
    if _target_db_engine is not None:
        return _target_db_engine
    
    pg_uri = os.getenv('MIRIX_PG_URI')
    if not pg_uri:
        # é»˜è®¤å›é€€ï¼ˆå¦‚æœç¯å¢ƒå˜é‡æ²¡é…ï¼‰
        logger.warning("æœªæ‰¾åˆ° MIRIX_PG_URIï¼Œå°è¯•ä½¿ç”¨é»˜è®¤é…ç½®...")
        pg_uri = 'postgresql+pg8000://aiop:G8CKsteyaWb#@pgsql01-share-rds-aliyun.item.pub:5432/mirix_pams'
    
    # éšè—å¯†ç æ‰“å°æ—¥å¿—
    safe_uri = pg_uri.split('@')[-1] if '@' in pg_uri else '***'
    logger.info(f"è¿æ¥ç›®æ ‡æ•°æ®åº“(PG): ...@{safe_uri}")
    
    try:
        # ä½¿ç”¨SQLAlchemyåˆ›å»ºå¼•æ“ï¼ˆå¸¦è¿æ¥æ± é…ç½®ï¼Œæ”¯æŒå¹¶å‘ï¼‰
        _target_db_engine = create_engine(
            pg_uri, 
            echo=False,
            pool_size=10,  # è¿æ¥æ± å¤§å°
            max_overflow=20,  # æœ€å¤§æº¢å‡ºè¿æ¥æ•°
            pool_pre_ping=True,  # è¿æ¥å‰pingæµ‹è¯•
            pool_recycle=3600  # 1å°æ—¶åå›æ”¶è¿æ¥
        )
        return _target_db_engine
    except Exception as e:
        logger.error(f"âœ— ç›®æ ‡æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
        raise


def init_learning_db():
    """æ£€æŸ¥å­¦ä¹ è®°å½•è¡¨æ˜¯å¦å­˜åœ¨ï¼ˆåœ¨ç›®æ ‡PGæ•°æ®åº“ï¼‰"""
    logger.info("æ­£åœ¨æ£€æŸ¥ ACE å­¦ä¹ è®°å½•è¡¨ (PostgreSQL)...")
    
    try:
        engine = get_target_db_engine()
        with engine.connect() as conn:
            # ç®€å•æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ï¼ˆä½¿ç”¨PostgreSQLçš„information_schemaï¼‰
            check_sql = text("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = 'public' 
                    AND table_name = 'ace_email_learning_records'
                );
            """)
            result = conn.execute(check_sql)
            table_exists = result.scalar()
            
            if table_exists:
                logger.info("âœ“ å­¦ä¹ è®°å½•è¡¨å·²å­˜åœ¨ï¼Œè·³è¿‡åˆå§‹åŒ–")
            else:
                logger.warning("âš ï¸ å­¦ä¹ è®°å½•è¡¨ä¸å­˜åœ¨ï¼è¯·æ‰‹åŠ¨æ‰§è¡Œ database/20251121_create_ace_learning_records.sql")
        
    except Exception as e:
        logger.warning(f"âš ï¸ æ£€æŸ¥è¡¨å¤±è´¥: {str(e)}ï¼Œç»§ç»­æ‰§è¡Œï¼ˆå‡è®¾è¡¨å·²å­˜åœ¨ï¼‰")


def save_learning_record(record_data: dict):
    """
    ä¿å­˜å•æ¡å­¦ä¹ è®°å½•åˆ°ç›®æ ‡æ•°æ®åº“ (PostgreSQL)
    æ³¨æ„ï¼šæ­¤å‡½æ•°æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œæ”¯æŒå¹¶å‘è°ƒç”¨
    """
    engine = get_target_db_engine()
    try:
        # æ„é€ SQL (ä½¿ç”¨SQLAlchemyçš„textå’Œå‚æ•°ç»‘å®š)
        sql = text("""
            INSERT INTO ace_email_learning_records (
                email_id, conversation_id, topic, mirix_data, 
                ground_truth, learned_strategies, final_score
            ) VALUES (:email_id, :conversation_id, :topic, :mirix_data, 
                      :ground_truth, :learned_strategies, :final_score)
        """)
        
        # å‡†å¤‡å‚æ•°
        params = {
            'email_id': str(record_data['email_id']),  # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²
            'conversation_id': record_data.get('conversation_id'),
            'topic': record_data.get('topic'),
            'mirix_data': json.dumps(record_data.get('workflow_data', {}), ensure_ascii=False), 
            'ground_truth': record_data.get('ground_truth'),
            'learned_strategies': json.dumps(record_data.get('learned_strategies', []), ensure_ascii=False),
            'final_score': float(record_data.get('final_score', 0.0))
        }
        
        # ä½¿ç”¨ç‹¬ç«‹çš„è¿æ¥ï¼ˆè¿æ¥æ± ä¼šè‡ªåŠ¨ç®¡ç†ï¼‰
        # æ¯ä¸ªå¹¶å‘ä»»åŠ¡éƒ½ä¼šä»è¿æ¥æ± è·å–ç‹¬ç«‹è¿æ¥ï¼Œäº’ä¸å¹²æ‰°
        with engine.connect() as conn:
            conn.execute(sql, params)
            conn.commit()  # æ˜¾å¼æäº¤äº‹åŠ¡
        
        logger.info(f"âœ“ å·²ä¿å­˜å­¦ä¹ è®°å½•åˆ° PG (Email ID: {record_data['email_id']})")
        
    except Exception as e:
        logger.error(f"âœ— ä¿å­˜å­¦ä¹ è®°å½•å¤±è´¥ (Email ID: {record_data.get('email_id', 'unknown')}): {str(e)}")
        import traceback
        logger.error(traceback.format_exc())


def fetch_email_by_id(email_id: str, user_id: int = 1952974833739087873):
    """
    æ ¹æ®email_idæŸ¥è¯¢å•ä¸ªé‚®ä»¶ä¼šè¯
    
    Args:
        email_id: é‚®ä»¶ID
        user_id: ç”¨æˆ·ID
    
    Returns:
        dict: åŒ…å« email_id, conversation_id, content çš„å­—å…¸ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
    """
    connection = get_source_db_connection()
    
    try:
        with connection.cursor() as cursor:
            sql = """
                SELECT
                    eb.id AS email_id,
                    eb.conversation_id,
                    eb.mail_type,
                    email_body.content_text
                FROM email_basic eb
                LEFT JOIN email_body ON email_body.email_basic_id = eb.id
                WHERE eb.id = %s
                  AND eb.user_id = %s
                  AND eb.mail_type = 'sent'
            """
            
            cursor.execute(sql, (email_id, user_id))
            result = cursor.fetchone()
            
            if not result:
                logger.warning(f"æœªæ‰¾åˆ°é‚®ä»¶ ID: {email_id}")
                return None
            
            return {
                'content': result['content_text'],
                'email_id': result['email_id'],
                'conversation_id': result['conversation_id']
            }
            
    except Exception as e:
        logger.error(f"âœ— æŸ¥è¯¢é‚®ä»¶å¤±è´¥: {str(e)}")
        raise
    finally:
        connection.close()


async def fetch_ticket_list_batch(staff_id: str = None, page: int = 1, page_size: int = 50) -> Tuple[List[Dict], bool]:
    """
    ä» ticket API è·å–ä¸€æ‰¹ ticket åˆ—è¡¨ï¼ˆæŒ‰é¡µè·å–ï¼ŒèŠ‚çœå†…å­˜ï¼‰
    
    Args:
        staff_id: å‘˜å·¥IDï¼Œé»˜è®¤ä½¿ç”¨é…ç½®çš„ DEFAULT_STAFF_ID
        page: é¡µç ï¼Œä»1å¼€å§‹
        page_size: æ¯é¡µæ•°é‡
    
    Returns:
        Tuple[List[Dict], bool]: (ticketåˆ—è¡¨, æ˜¯å¦è¿˜æœ‰æ›´å¤šé¡µ)
    """
    if staff_id is None:
        staff_id = DEFAULT_STAFF_ID
    
    url = f"{TICKET_API_BASE_URL}/tickets/page"
    headers = {
        "x-api-key": TICKET_API_KEY,
        "Content-Type": "application/json"
    }
    
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            payload = {
                "size": page_size,
                "page": page,
                "input": {
                    "displayStatusIds": [2],  # å·²è§£å†³
                    "staffIds": [staff_id]
                }
            }
            
            logger.info(f"æ­£åœ¨è·å– ticket åˆ—è¡¨ (ç¬¬ {page} é¡µï¼Œæ¯é¡µ {page_size} æ¡)...")
            response = await client.post(url, json=payload, headers=headers)
            response.raise_for_status()
            
            result = response.json()
            if result.get("code") != 200:
                logger.error(f"APIè¿”å›é”™è¯¯: {result.get('msg')}")
                return [], False
            
            data = result.get("data", {})
            tickets = data.get("records", [])
            
            # åˆ¤æ–­æ˜¯å¦è¿˜æœ‰æ›´å¤šé¡µï¼šå¦‚æœè¿”å›çš„æ•°é‡ç­‰äºè¯·æ±‚çš„æ•°é‡ï¼Œå¯èƒ½è¿˜æœ‰æ›´å¤š
            has_more = len(tickets) == page_size
            
            logger.info(f"âœ“ å·²è·å– {len(tickets)} ä¸ª tickets")
            return tickets, has_more
        
    except Exception as e:
        logger.error(f"âœ— è·å– ticket åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise


async def fetch_ticket_messages(ticket_id: str) -> List[Dict]:
    """
    è·å–æŒ‡å®š ticket çš„æ¶ˆæ¯åˆ—è¡¨
    
    Args:
        ticket_id: ticket ID
    
    Returns:
        List[Dict]: æ¶ˆæ¯åˆ—è¡¨ï¼ˆæŒ‰æ—¶é—´å€’åºï¼‰
    """
    url = f"{TICKET_API_BASE_URL}/tickets/{ticket_id}/messages"
    headers = {
        "x-api-key": TICKET_API_KEY,
        "Content-Type": "application/json"
    }
    
    all_messages = []
    page = 1
    page_size = 100
    
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            while True:
                payload = {
                    "size": page_size,
                    "page": page,
                    "orders": [{
                        "column": "id",
                        "asc": False  # æŒ‰IDå€’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
                    }],
                    "input": {
                        "types": [1, 3, 5]  # 1 é‚®ä»¶æ¶ˆæ¯, 3 å†…éƒ¨æ¶ˆæ¯, 5 å…¬å¼€å›å¤
                    }
                }
                
                response = await client.post(url, json=payload, headers=headers)
                response.raise_for_status()
                
                result = response.json()
                if result.get("code") != 200:
                    logger.error(f"APIè¿”å›é”™è¯¯: {result.get('msg')}")
                    break
                
                data = result.get("data", {})
                messages = data.get("records", [])
                
                if not messages:
                    break
                
                all_messages.extend(messages)
                
                # å¦‚æœè¿”å›çš„æ•°é‡å°äºè¯·æ±‚çš„æ•°é‡ï¼Œè¯´æ˜å·²ç»æ˜¯æœ€åä¸€é¡µ
                if len(messages) < page_size:
                    break
                
                page += 1
        
        logger.info(f"âœ“ è·å–åˆ° ticket {ticket_id} çš„ {len(all_messages)} æ¡æ¶ˆæ¯")
        return all_messages
        
    except Exception as e:
        logger.error(f"âœ— è·å– ticket {ticket_id} æ¶ˆæ¯å¤±è´¥: {str(e)}")
        raise


def build_ticket_conversation_thread(messages: List[Dict], staff_email: str = None, staff_name: str = None) -> Tuple[List[Dict], Optional[Dict]]:
    """
    é€šè¿‡ replyMessageId æ„å»ºå®Œæ•´çš„å¯¹è¯çº¿ç¨‹ï¼Œå¹¶è¯†åˆ« ground_truth
    
    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨ï¼ˆå·²æŒ‰æ—¶é—´å€’åºï¼‰
        staff_email: CSRé‚®ç®±ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®çš„ DEFAULT_STAFF_EMAIL
        staff_name: CSRå§“åï¼Œé»˜è®¤ä½¿ç”¨é…ç½®çš„ DEFAULT_STAFF_NAME
    
    Returns:
        Tuple[List[Dict], Optional[Dict]]: (å¯¹è¯å†å²åˆ—è¡¨, ground_truthæ¶ˆæ¯)
        - å¦‚æœæ‰¾åˆ° CSR å›å¤ï¼Œground_truth_msg æ˜¯ CSR çš„å›å¤æ¶ˆæ¯
        - å¦‚æœæ²¡æœ‰æ‰¾åˆ° CSR å›å¤ï¼Œground_truth_msg æ˜¯ä¸€ä¸ªç‰¹æ®Šçš„ resolved æ ‡è®°æ¶ˆæ¯ï¼ˆis_resolved=Trueï¼‰
        - å¦‚æœæ²¡æœ‰ä»»ä½•æ¶ˆæ¯ï¼Œè¿”å› ([], None)
    """
    if staff_email is None:
        staff_email = DEFAULT_STAFF_EMAIL
    if staff_name is None:
        staff_name = DEFAULT_STAFF_NAME
    
    # è¿‡æ»¤æ‰ Atlas çš„æ¶ˆæ¯
    filtered_messages = [
        msg for msg in messages
        if msg.get("userName", "").lower() != "atlas" 
        and msg.get("userEmail", "").lower() != "atlas"
    ]
    
    if not filtered_messages:
        logger.warning("è¿‡æ»¤åæ²¡æœ‰æ¶ˆæ¯")
        return [], None
    
    # æ„å»ºæ¶ˆæ¯æ˜ å°„ï¼ˆid -> messageï¼‰
    message_map = {str(msg.get("id")): msg for msg in filtered_messages}
    
    # æ„å»ºåå‘æ˜ å°„ï¼ˆreplyMessageId -> [messages that reply to it]ï¼‰
    reply_map = {}
    for msg in filtered_messages:
        reply_id = msg.get("replyMessageId")
        if reply_id:
            reply_id_str = str(reply_id)
            if reply_id_str not in reply_map:
                reply_map[reply_id_str] = []
            reply_map[reply_id_str].append(msg)
    
    # æ‰¾åˆ°æ ¹æ¶ˆæ¯ï¼ˆæ²¡æœ‰ replyMessageId æˆ– replyMessageId ä¸åœ¨å½“å‰æ¶ˆæ¯åˆ—è¡¨ä¸­çš„ï¼‰
    root_messages = []
    for msg in filtered_messages:
        reply_id = msg.get("replyMessageId")
        if not reply_id or str(reply_id) not in message_map:
            root_messages.append(msg)
    
    # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„æ ¹æ¶ˆæ¯ï¼Œä½¿ç”¨ç¬¬ä¸€æ¡æ¶ˆæ¯ï¼ˆæœ€æ—§çš„ï¼‰
    if not root_messages:
        # æŒ‰æ—¶é—´æ’åºï¼Œæœ€æ—§çš„åœ¨å‰é¢
        sorted_messages = sorted(filtered_messages, key=lambda x: x.get("createTime", ""))
        if sorted_messages:
            root_messages = [sorted_messages[0]]
    
    # ä»æ ¹æ¶ˆæ¯å¼€å§‹ï¼Œé€šè¿‡ replyMessageId é€’å½’æ„å»ºçº¿ç¨‹
    def build_thread_from_root(root_msg: Dict) -> List[Dict]:
        thread = [root_msg]
        root_id = str(root_msg.get("id"))
        
        # æ‰¾åˆ°æ‰€æœ‰å›å¤è¿™æ¡æ¶ˆæ¯çš„æ¶ˆæ¯
        if root_id in reply_map:
            for reply_msg in reply_map[root_id]:
                # é€’å½’æ„å»ºå­çº¿ç¨‹
                sub_thread = build_thread_from_root(reply_msg)
                thread.extend(sub_thread)
        
        return thread
    
    # æ„å»ºæ‰€æœ‰çº¿ç¨‹
    all_threads = []
    processed_ids = set()
    
    for root in root_messages:
        if str(root.get("id")) not in processed_ids:
            thread = build_thread_from_root(root)
            all_threads.extend(thread)
            processed_ids.update(str(msg.get("id")) for msg in thread)
    
    # å¦‚æœè¿˜æœ‰æœªå¤„ç†çš„æ¶ˆæ¯ï¼ˆå¯èƒ½æ˜¯ç‹¬ç«‹çš„çº¿ç¨‹ï¼‰ï¼Œä¹ŸåŠ å…¥
    for msg in filtered_messages:
        msg_id = str(msg.get("id"))
        if msg_id not in processed_ids:
            all_threads.append(msg)
            processed_ids.add(msg_id)
    
    # æŒ‰æ—¶é—´é¡ºåºæ’åºï¼ˆæœ€æ—§çš„åœ¨å‰é¢ï¼‰
    all_threads.sort(key=lambda x: x.get("createTime", ""))
    
    # è¯†åˆ« ground_truthï¼šæ£€æŸ¥æ¶ˆæ¯æ˜¯å¦åŒ…å« CSR ä¿¡æ¯ï¼ˆuserEmail æˆ– recipients.fromï¼‰
    ground_truth_msg = None
    is_resolved = False
    
    def is_csr_message(msg: Dict) -> bool:
        """
        åˆ¤æ–­æ¶ˆæ¯æ˜¯å¦æ˜¯ CSR å‘å‡ºçš„
        æ£€æŸ¥ userEmail å’Œ recipients.from æ˜¯å¦åŒ…å« CSR ä¿¡æ¯
        """
        user_email = msg.get("userEmail", "").lower()
        user_name = msg.get("userName", "")
        
        # æ£€æŸ¥ userEmail æ˜¯å¦åŒ¹é…
        if user_email == staff_email.lower() or user_name == staff_name:
            return True
        
        # æ£€æŸ¥ recipients.from æ˜¯å¦åŒ…å« CSR ä¿¡æ¯
        recipients = msg.get("recipients", {})
        recipients_from = recipients.get("from", [])
        if recipients_from:
            # recipients_from å¯èƒ½æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨æˆ–å•ä¸ªå­—ç¬¦ä¸²
            recipients_from_str = ', '.join(recipients_from) if isinstance(recipients_from, list) else str(recipients_from)
            recipients_from_lower = recipients_from_str.lower()
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å« CSR é‚®ç®±æˆ–å§“å
            if staff_email.lower() in recipients_from_lower or staff_name.lower() in recipients_from_lower:
                return True
        
        return False
    id = msg.get("id", "").lower()
    ticketId = msg.get("ticketId", "").lower()
    # ä»æœ€æ–°æ¶ˆæ¯å¼€å§‹æŸ¥æ‰¾ CSR çš„å›å¤
    for msg in reversed(all_threads):
        if is_csr_message(msg):
            ground_truth_msg = msg

            logger.info(f"âœ“ æ‰¾åˆ° CSR å›å¤ä½œä¸º ground_truth (æ¶ˆæ¯ID: {msg.get('id')}, userEmail: {msg.get('userEmail')}, recipients.from: {msg.get('recipients', {}).get('from', [])})")
            break
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ° CSR å›å¤ï¼Œæ ‡è®°ä¸º resolvedï¼ˆè¿™ä¹Ÿæ˜¯ ground_truth çš„ä¸€ç§ï¼‰
    if not ground_truth_msg:
        is_resolved = True
        logger.info("âœ“ æœªæ‰¾åˆ° CSR å›å¤ï¼Œæ ‡è®°ä¸º resolvedï¼ˆè¿™ä¹Ÿæ˜¯ ground_truth çš„ä¸€ç§ï¼‰")
        # åˆ›å»ºä¸€ä¸ªç‰¹æ®Šçš„ resolved æ ‡è®°æ¶ˆæ¯
        ground_truth_msg = {
            'id': id,
            'ticketId': ticketId,
            'is_resolved': True,
            'content': 'æ­¤ ticket ä¸éœ€è¦ CSR å¤„ç†ï¼Œå·²æ ‡è®°ä¸º resolved',
            'userName': staff_name,
            'userEmail': staff_email,
            'createTime': all_threads[-1].get('createTime', '') if all_threads else ''
        }
    
    # æ„å»ºå†å²ï¼ˆæ’é™¤ ground_truthï¼Œä½†å¦‚æœæ˜¯ resolvedï¼Œåˆ™åŒ…å«æ‰€æœ‰æ¶ˆæ¯ä½œä¸ºå†å²ï¼‰
    if is_resolved:
        history = all_threads  # resolved æƒ…å†µä¸‹ï¼Œæ‰€æœ‰æ¶ˆæ¯éƒ½æ˜¯å†å²
    else:
        history = [msg for msg in all_threads if msg != ground_truth_msg]
    
    return history, ground_truth_msg


# å…¨å±€è½¬æ¢å™¨å®ä¾‹ï¼ˆå¤ç”¨ï¼‰
_html_converter = None

def get_html_converter() -> HtmlToMarkdownConverter:
    """è·å–å…¨å±€ HTML è½¬ Markdown è½¬æ¢å™¨å®ä¾‹"""
    global _html_converter
    if _html_converter is None:
        _html_converter = HtmlToMarkdownConverter()
    return _html_converter


def format_ticket_message_for_training(msg: Dict, ticket_title: str = "") -> str:
    """
    æ ¼å¼åŒ– ticket æ¶ˆæ¯ä¸ºè®­ç»ƒç”¨çš„æ–‡æœ¬æ ¼å¼
    
    Args:
        msg: æ¶ˆæ¯å­—å…¸
        ticket_title: ticket æ ‡é¢˜
    
    Returns:
        str: æ ¼å¼åŒ–åçš„æ–‡æœ¬
    """
    # æå– HTML å†…å®¹å¹¶è½¬æ¢ä¸º Markdown
    html_content = msg.get("content", "")
    
    # ä½¿ç”¨ HtmlToMarkdownConverter è½¬æ¢ä¸º Markdown
    converter = get_html_converter()
    content = converter.convert_html_to_md(html_content)
    
    user_name = msg.get("userName", "Unknown")
    user_email = msg.get("userEmail", "")
    create_time = msg.get("createTime", "")
    
    # è·å–æ”¶ä»¶äººä¿¡æ¯
    recipients = msg.get("recipients", {})
    recipients_from = recipients.get("from", [])
    recipients_to = recipients.get("to", [])
    recipients_cc = recipients.get("cc", [])
    
    formatted = f"å‘ä»¶äºº: {user_name}"
    if user_email:
        formatted += f" <{user_email}>"
    formatted += f"\næ—¶é—´: {create_time}\n"
    
    if recipients_from:
        formatted += f"From: {', '.join(recipients_from)}\n"
    if recipients_to:
        formatted += f"To: {', '.join(recipients_to)}\n"
    if recipients_cc:
        formatted += f"CC: {', '.join(recipients_cc)}\n"
    
    if ticket_title:
        formatted += f"ä¸»é¢˜: {ticket_title}\n"
    
    formatted += f"\n{content}\n"
    
    return formatted


def process_ticket_conversation(ticket: Dict, messages: List[Dict], staff_email: str = None, staff_name: str = None) -> Optional[Dict]:
    """
    å¤„ç†å•ä¸ª ticket çš„å¯¹è¯ï¼Œæå–è®­ç»ƒæ•°æ®
    
    Args:
        ticket: ticket ä¿¡æ¯
        messages: ticket çš„æ¶ˆæ¯åˆ—è¡¨
        staff_email: CSRé‚®ç®±
        staff_name: CSRå§“å
    
    Returns:
        Optional[Dict]: å¤„ç†ç»“æœï¼ŒåŒ…å« conversation_id, content, email_id ç­‰
        - åŒ…å« is_resolved å­—æ®µï¼Œè¡¨ç¤ºæ˜¯å¦ä¸º resolved ç±»å‹çš„ ground_truth
        - å¦‚æœæ²¡æœ‰ä»»ä½•æœ‰æ•ˆæ¶ˆæ¯ï¼Œè¿”å› None
    """
    ticket_id = ticket.get("id")
    ticket_title = ticket.get("title", "")
    
    # æ„å»ºå¯¹è¯çº¿ç¨‹
    history, ground_truth_msg = build_ticket_conversation_thread(messages, staff_email, staff_name)
    
    if not ground_truth_msg:
        logger.warning(f"Ticket {ticket_id} æ²¡æœ‰æœ‰æ•ˆçš„æ¶ˆæ¯")
        return None
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ resolved æƒ…å†µ
    is_resolved = ground_truth_msg.get('is_resolved', False)
    
    # æ ¼å¼åŒ–å†å²æ¶ˆæ¯
    history_text = ""
    for msg in history:
        history_text += format_ticket_message_for_training(msg, ticket_title)
        history_text += "\n" + "-" * 60 + "\n"
    
    # æ ¼å¼åŒ– ground_truth
    if is_resolved:
        # resolved æƒ…å†µçš„ç‰¹æ®Šæ ¼å¼
        ground_truth_text = f"""å‘ä»¶äºº: {staff_name} <{staff_email}>
æ—¶é—´: {ground_truth_msg.get('createTime', '')}
ä¸»é¢˜: {ticket_title}

æ­¤ ticket ä¸éœ€è¦ CSR å¤„ç†ï¼Œå·²æ ‡è®°ä¸º resolvedã€‚
"""
    else:
        # æ­£å¸¸ CSR å›å¤çš„æ ¼å¼
        ground_truth_text = format_ticket_message_for_training(ground_truth_msg, ticket_title)
    
    # ç»„åˆå®Œæ•´å†…å®¹ï¼ˆground_truth åœ¨å‰ï¼Œhistory åœ¨åï¼Œç”¨äº LLM å¤„ç†ï¼‰
    full_content = ground_truth_text + "\n" + "=" * 60 + "\nå†å²å¯¹è¯:\n" + history_text
    
    return {
        'content': full_content,
        'email_id': str(ticket_id),  # ä½¿ç”¨ ticket_id ä½œä¸ºå”¯ä¸€æ ‡è¯†
        'conversation_id': str(ticket_id),  # threadId ä½œä¸º ticket çš„å”¯ä¸€ä¸»é”®
        'ticket_id': ticket_id,
        'ticket_title': ticket_title,
        'ground_truth_raw': ground_truth_text,
        'history_raw': history_text,
        'email_account': staff_email,  # æ·»åŠ é‚®ç®±è´¦æˆ·ä¿¡æ¯ï¼Œç”¨äº workflow API è°ƒç”¨
        'is_resolved': is_resolved  # æ ‡è®°æ˜¯å¦ä¸º resolved æƒ…å†µ
    }


async def process_ticket_batch(
    tickets: List[Dict],
    staff_email: str,
    staff_name: str,
    batch_num: int
) -> List[Dict]:
    """
    å¤„ç†ä¸€æ‰¹ ticketsï¼Œè·å–æ¶ˆæ¯å¹¶è½¬æ¢ä¸ºè®­ç»ƒæ•°æ®
    
    Args:
        tickets: ticket åˆ—è¡¨
        staff_email: CSRé‚®ç®±
        staff_name: CSRå§“å
        batch_num: æ‰¹æ¬¡ç¼–å·
    
    Returns:
        List[Dict]: å¤„ç†åçš„ä¼šè¯åˆ—è¡¨
    """
    conversations_list = []
    
    logger.info(f"\nå¤„ç†ç¬¬ {batch_num} æ‰¹ï¼Œå…± {len(tickets)} ä¸ª tickets...")
    
    for idx, ticket in enumerate(tickets, 1):
        ticket_id = ticket.get("id")
        logger.info(f"  å¤„ç† ticket {idx}/{len(tickets)} (ID: {ticket_id})...")
        
        try:
            # è·å–æ¶ˆæ¯è¯¦æƒ…
            messages = await fetch_ticket_messages(ticket_id)
            
            if not messages:
                logger.warning(f"    Ticket {ticket_id} æ²¡æœ‰æ¶ˆæ¯ï¼Œè·³è¿‡")
                continue
            
            # å¤„ç†å¯¹è¯
            conv_data = process_ticket_conversation(ticket, messages, staff_email, staff_name)
            
            if conv_data:
                conversations_list.append(conv_data)
                if conv_data.get('is_resolved'):
                    logger.info(f"    âœ“ Ticket {ticket_id} å¤„ç†æˆåŠŸï¼ˆresolved ç±»å‹ï¼‰")
                else:
                    logger.info(f"    âœ“ Ticket {ticket_id} å¤„ç†æˆåŠŸï¼ˆCSR å›å¤ç±»å‹ï¼‰")
            else:
                logger.warning(f"    - Ticket {ticket_id} è·³è¿‡ï¼ˆæ²¡æœ‰æœ‰æ•ˆæ¶ˆæ¯ï¼‰")
        
        except Exception as e:
            logger.error(f"    âœ— Ticket {ticket_id} å¤„ç†å¤±è´¥: {str(e)}")
            continue
    
    logger.info(f"  âœ“ ç¬¬ {batch_num} æ‰¹å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆä¼šè¯: {len(conversations_list)} ä¸ª")
    return conversations_list


async def fetch_ticket_conversations_from_api_batch(
    staff_id: str = None,
    staff_email: str = None,
    staff_name: str = None,
    max_tickets: int = None,
    batch_size: int = None
):
    """
    ä» ticket API æŒ‰æ‰¹æ¬¡è·å– ticket ä¼šè¯åˆ—è¡¨ï¼ˆç”Ÿæˆå™¨ï¼ŒèŠ‚çœå†…å­˜ï¼‰
    
    Args:
        staff_id: å‘˜å·¥ID
        staff_email: CSRé‚®ç®±
        staff_name: CSRå§“å
        max_tickets: æœ€å¤§ticketæ•°é‡
        batch_size: æ¯æ‰¹å¤„ç†çš„ticketæ•°é‡
    
    Yields:
        List[Dict]: æ¯æ‰¹å¤„ç†åçš„ä¼šè¯åˆ—è¡¨
    """
    if staff_id is None:
        staff_id = DEFAULT_STAFF_ID
    if staff_email is None:
        staff_email = DEFAULT_STAFF_EMAIL
    if staff_name is None:
        staff_name = DEFAULT_STAFF_NAME
    if max_tickets is None:
        max_tickets = MAX_TICKETS
    if batch_size is None:
        batch_size = BATCH_SIZE
    
    logger.info(f"å¼€å§‹ä» ticket API æŒ‰æ‰¹æ¬¡è·å–æ•°æ®...")
    logger.info(f"  å‘˜å·¥ID: {staff_id}")
    logger.info(f"  å‘˜å·¥é‚®ç®±: {staff_email}")
    logger.info(f"  å‘˜å·¥å§“å: {staff_name}")
    logger.info(f"  æœ€å¤§æ•°é‡: {max_tickets}")
    logger.info(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
    
    page = 1
    page_size = batch_size  # æ¯é¡µè·å–çš„æ•°é‡ç­‰äºæ‰¹æ¬¡å¤§å°
    total_processed = 0
    batch_num = 1
    
    while total_processed < max_tickets:
        # è·å–ä¸€æ‰¹ tickets
        tickets, has_more = await fetch_ticket_list_batch(staff_id, page, page_size)
        
        if not tickets:
            logger.info("æ²¡æœ‰æ›´å¤š ticketsï¼Œç»“æŸ")
            break
        
        # å¤„ç†è¿™æ‰¹ tickets
        conversations_list = await process_ticket_batch(tickets, staff_email, staff_name, batch_num)
        
        if conversations_list:
            yield conversations_list
        
        total_processed += len(tickets)
        batch_num += 1
        
        # å¦‚æœå·²è¾¾åˆ°æœ€å¤§æ•°é‡æˆ–æ²¡æœ‰æ›´å¤šé¡µï¼Œç»“æŸ
        if total_processed >= max_tickets or not has_more:
            break
        
        page += 1
    
    logger.info(f"\nâœ“ æ€»å…±å¤„ç† {total_processed} ä¸ª ticketsï¼Œåˆ† {batch_num - 1} æ‰¹")


def fetch_email_conversations_from_db(user_id: int = 1952974833739087873, limit: int = 10, offset: int = 0):
    """
    ä»æºæ•°æ®åº“(MySQL)æŸ¥è¯¢é‚®ä»¶ä¼šè¯
    """
    connection = get_source_db_connection()
    
    try:
        with connection.cursor() as cursor:
            # ä¸¥æ ¼æ‰§è¡Œç”¨æˆ·æä¾›çš„SQLï¼ˆæ¯æ¡è®°å½•çš„content_textå·²åŒ…å«å®Œæ•´ä¼šè¯ï¼‰
            sql = """
                WITH s AS (
                    SELECT eb.*
                    FROM email_basic eb
                    WHERE eb.user_id = %s
                      AND eb.mail_type = 'sent'
                      AND eb.conversation_id IS NOT NULL
                ),
                r AS (
                    SELECT s.*, ROW_NUMBER() OVER (
                        PARTITION BY s.conversation_id
                        ORDER BY s.sent_date_time DESC
                    ) AS rn
                    FROM s
                ),
                latest_sent AS (
                    SELECT *
                    FROM r
                    WHERE rn = 1
                )
                SELECT
                    ls.id              AS email_id,
                    ls.conversation_id,
                    ls.mail_type,
                    eb.content_text
                FROM latest_sent ls
                LEFT JOIN email_body eb ON eb.email_basic_id = ls.id
                WHERE EXISTS (
                    SELECT 1
                    FROM email_basic x
                    LEFT JOIN email_body xb ON xb.email_basic_id = x.id
                    WHERE x.user_id = ls.user_id
                      AND x.conversation_id = ls.conversation_id
                      AND x.mail_type = 'received'
                      AND COALESCE(x.received_date_time, x.created_date_time, x.created_at)
                          <= COALESCE(ls.sent_date_time, ls.created_date_time, ls.created_at)
                      AND (
                          (x.internet_message_id IS NULL OR ls.internet_message_id IS NULL 
                           OR x.internet_message_id <> ls.internet_message_id)
                          OR (MD5(xb.content_text) IS NULL OR MD5(eb.content_text) IS NULL 
                              OR MD5(xb.content_text) <> MD5(eb.content_text))
                      )
                )
                ORDER BY ls.sent_date_time DESC
                LIMIT %s OFFSET %s
            """
            
            cursor.execute(sql, (user_id, limit, offset))
            results = cursor.fetchall()
            
            if not results:
                logger.info("âœ“ æœªæŸ¥è¯¢åˆ°ç¬¦åˆæ¡ä»¶çš„é‚®ä»¶ä¼šè¯")
                return []
            
            logger.info(f"âœ“ æŸ¥è¯¢åˆ° {len(results)} ä¸ªä¼šè¯")
            
            conversations_list = []
            for row in results:
                conversations_list.append({
                    'content': row['content_text'],
                    'email_id': row['email_id'],
                    'conversation_id': row['conversation_id']
                })
                
            logger.info(f"âœ“ å‡†å¤‡è¿›è¡Œè®­ç»ƒï¼Œå…± {len(conversations_list)} ä¸ªä¼šè¯")
            return conversations_list
            
    except Exception as e:
        logger.error(f"âœ— æŸ¥è¯¢é‚®ä»¶æ•°æ®å¤±è´¥: {str(e)}")
        raise
    finally:
        connection.close()


class EmailTaskEnvironment(TaskEnvironment):
    """ACEè®­ç»ƒç¯å¢ƒï¼ˆä½¿ç”¨EmailEvaluationAgentè¯„ä¼°ï¼‰"""
    
    def __init__(self, evaluation_agent: ACEEvaluationAgent):
        self.evaluation_agent = evaluation_agent
    
    def evaluate(self, sample: Sample, generated_output) -> EnvironmentResult:
        """
        ACEè¦æ±‚å®ç°çš„è¯„ä¼°æ–¹æ³•
        """
        # è§£æcontextï¼ˆåŒ…å«åŸå§‹é‚®ä»¶ã€workflowå’Œå†å²ï¼‰
        email_context = json.loads(sample.context) if isinstance(sample.context, str) else sample.context
        
        # è·å–ç”Ÿæˆçš„æœ€ç»ˆç­”æ¡ˆ
        final_answer = generated_output.final_answer
        
        # è°ƒç”¨è¯„ä¼°Agentï¼ˆåŒæ­¥è°ƒç”¨ï¼‰
        result = self.evaluation_agent.evaluate_reply(
            generated_reply=final_answer,
            ground_truth_reply=sample.ground_truth,
            email_context=email_context
        )
        
        return result


async def call_workflow_extract_api(email_content: str, email_account: str = "test@example.com") -> dict:
    """è°ƒç”¨çœŸå®çš„ /workflow/extract æ¥å£è·å–workflow"""
    url = "https://aiop-dev.item.pub/pams/workflow/extract"
    
    payload = {
        "content": email_content,
        "email_account": email_account
    }
    
    try:
        # è¶…æ—¶è®¾ç½®ä¸º180ç§’ï¼ˆ3åˆ†é’Ÿï¼‰
        async with httpx.AsyncClient(timeout=180.0) as client:
            logger.info(f"è°ƒç”¨workflowæå–API: {url}")
            
            response = await client.post(
                url,
                json=payload,
                headers={"Content-Type": "application/json"}
            )
            
            response.raise_for_status()
            result = response.json()
            
            logger.info(f"âœ“ workflowæå–æˆåŠŸ")
            return result.get("workflow_result", result)
    
    except httpx.TimeoutException as e:
        logger.warning(f"  workflowæå–è¶…æ—¶ï¼ˆ180ç§’ï¼‰ï¼Œè·³è¿‡")
        return {"workflow_type": "unknown", "reasoning": "APIè°ƒç”¨è¶…æ—¶"}
    
    except httpx.HTTPError as e:
        logger.warning(f"  workflowæå–å¤±è´¥: {str(e)}ï¼Œè·³è¿‡")
        return {"workflow_type": "unknown", "reasoning": f"APIè°ƒç”¨å¤±è´¥: {str(e)}"}
    
    except Exception as e:
        logger.warning(f"  workflowæå–å¼‚å¸¸: {str(e)}ï¼Œè·³è¿‡")
        return {"workflow_type": "unknown", "reasoning": f"APIè°ƒç”¨å¼‚å¸¸: {str(e)}"}


async def preprocess_ground_truth_to_steps(natural_text: str, llm_client) -> str:
    """ä½¿ç”¨LLMå°†è‡ªç„¶å¯¹è¯æ ¼å¼çš„é‚®ä»¶è½¬æ¢ä¸ºä¸¥æ ¼æ­¥éª¤åŒ–æ ¼å¼"""
    preprocessing_prompt = f"""è¯·å°†ä»¥ä¸‹è‡ªç„¶å¯¹è¯é£æ ¼çš„é‚®ä»¶å›å¤ï¼Œæ”¹å†™ä¸ºä¸¥æ ¼çš„æ­¥éª¤åŒ–æ ¼å¼ã€‚

ä¸¥æ ¼è¦æ±‚ï¼š
1. **å¿…é¡»ä¿ç•™æ‰€æœ‰æŠ€æœ¯ç»†èŠ‚**ï¼š
   - äººåï¼ˆå¦‚Francesã€Anthonyã€Jeffã€Eadenç­‰ï¼‰
   - ç³»ç»Ÿåï¼ˆå¦‚WMSã€EDIã€APIã€CubeShipç­‰ï¼‰
   - é…ç½®å€¼ï¼ˆå¦‚ISA IDã€POå·ç ç­‰ï¼‰
   - å›¢é˜Ÿåï¼ˆå¦‚Jolietå›¢é˜Ÿã€B-Solutionsç­‰ï¼‰

2. **æ ¼å¼è¦æ±‚**ï¼š
   - ä½¿ç”¨"ç¬¬ä¸€æ­¥ï¼š..."ã€"ç¬¬äºŒæ­¥ï¼š..."ã€"ç¬¬ä¸‰æ­¥ï¼š..."æ ¼å¼
   - æ¯ä¸ªæ­¥éª¤ç”¨ä¸€å¥å®Œæ•´çš„è¯æè¿°è¦åšçš„äº‹æƒ…
   - æ­¥éª¤ä¹‹é—´ç©ºä¸€è¡Œ
   - ä¸è¦ä½¿ç”¨"-"ã€"â€¢"ç­‰åˆ—è¡¨ç¬¦å·
   - ä¸è¦åœ¨æ­¥éª¤ä¸‹å†åˆ†å­è¦ç‚¹

3. **ä¿æŒåŸæœ‰å†…å®¹**ï¼š
   - ä¿æŒå¼€å¤´å’Œç»“å°¾çš„é—®å€™è¯­
   - ä¿æŒæ—¶é—´é¢„ä¼°
   - ä¿æŒç­¾å

åŸå§‹é‚®ä»¶ï¼š
{natural_text}

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼è¦æ±‚è¾“å‡ºæ”¹å†™åçš„é‚®ä»¶ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–é¢å¤–è¯´æ˜ã€‚"""
    
    logger.info("\n[é¢„å¤„ç†] ä½¿ç”¨LLMå°†è‡ªç„¶å¯¹è¯è½¬æ¢ä¸ºæ­¥éª¤åŒ–æ ¼å¼...")
    response = llm_client.complete(preprocessing_prompt)
    processed_text = response.text.strip()
    
    logger.info("=" * 80)
    logger.info("âœ“ é¢„å¤„ç†å®Œæˆï¼LLMè½¬æ¢åçš„é‚®ä»¶å†…å®¹ï¼š")
    logger.info("=" * 80)
    logger.info(processed_text)
    logger.info("=" * 80)
    
    return processed_text


def summarize_long_email(raw_emails: str, llm_client) -> str:
    """å¯¹è¶…é•¿é‚®ä»¶è¿›è¡Œæ™ºèƒ½æ€»ç»“ï¼Œä¿ç•™å…³é”®ä¿¡æ¯"""
    logger.info(f"  é‚®ä»¶è¿‡é•¿ï¼ˆ{len(raw_emails)}å­—ç¬¦ï¼‰ï¼Œå…ˆè¿›è¡Œæ™ºèƒ½æ€»ç»“...")
    
    summary_prompt = f"""è¿™æ˜¯ä¸€å°å¾ˆé•¿çš„é‚®ä»¶çº¿ç¨‹ã€‚è¯·æ€»ç»“å…³é”®ä¿¡æ¯ï¼Œä¿æŒç»“æ„æ¸…æ™°ã€‚

é‚®ä»¶å†…å®¹ï¼š
{raw_emails[:50000]}

æ€»ç»“è¦æ±‚ï¼š
1. ä¿ç•™æœ€æ–°çš„å›å¤å†…å®¹ï¼ˆå®Œæ•´ï¼‰
2. ä¿ç•™æ‰€æœ‰äººåã€å…¬å¸åã€ç³»ç»Ÿåã€è®¢å•å·ã€é…ç½®å€¼ç­‰å…³é”®ä¿¡æ¯
3. ä¿ç•™æ—¶é—´çº¿å’Œå¯¹è¯æµç¨‹
4. å‹ç¼©é‡å¤å†…å®¹å’Œå†—ä½™çš„ç­¾å/å£°æ˜
5. ä¿æŒ"å‘ä»¶äºº:"/"From:"ç­‰åˆ†éš”ç¬¦
6. ç›®æ ‡é•¿åº¦ï¼šä¸è¶…è¿‡20000å­—ç¬¦

è¯·è¾“å‡ºæ€»ç»“åçš„é‚®ä»¶å†…å®¹ï¼š"""
    
    try:
        response = llm_client.complete(summary_prompt)
        summarized = response.text.strip()
        logger.info(f"  âœ“ æ€»ç»“å®Œæˆï¼Œå‹ç¼©åˆ° {len(summarized)} å­—ç¬¦")
        return summarized
    except Exception as e:
        logger.warning(f"  æ€»ç»“å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨æˆªæ–­æ–¹å¼")
        return raw_emails[:30000]


def process_conversation_with_llm(emails_data: list, llm_client, retry_count: int = 0) -> dict:
    """ä½¿ç”¨LLMæ™ºèƒ½å¤„ç†é‚®ä»¶ä¼šè¯"""
    if not emails_data:
        raise ValueError("é‚®ä»¶æ•°æ®ä¸ºç©º")
    
    raw_emails = emails_data[0] if emails_data else ""
    
    logger.info(f"\n[LLMå¤„ç†ä¼šè¯] é‚®ä»¶å†…å®¹é•¿åº¦: {len(raw_emails)} å­—ç¬¦")
    
    if len(raw_emails) > 30000:
        raw_emails = summarize_long_email(raw_emails, llm_client)
    
    processing_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„é‚®ä»¶åˆ†æåŠ©æ‰‹ã€‚è¯·ä»è¿™å°å·²å‘é€çš„é‚®ä»¶ä¸­æå–è®­ç»ƒæ‰€éœ€çš„ä¿¡æ¯ã€‚

ã€é‚®ä»¶å†…å®¹ã€‘
{raw_emails}

ã€ä»»åŠ¡è¯´æ˜ã€‘
è¿™å°é‚®ä»¶åŒ…å«ï¼š
- æœ€æ–°å›å¤å†…å®¹ï¼ˆå¼€å¤´åˆ°ç¬¬ä¸€ä¸ª"å‘ä»¶äºº:"/"From:"ä¹‹å‰ï¼‰
- å†å²é‚®ä»¶å¯¹è¯ï¼ˆä»"å‘ä»¶äºº:"/"From:"å¼€å§‹çš„éƒ¨åˆ†ï¼‰

ã€è¾“å‡ºè¦æ±‚ - å¿…é¡»ä¸¥æ ¼éµå®ˆã€‘
ä½ å¿…é¡»è¾“å‡ºå®Œæ•´çš„XMLæ ¼å¼ï¼ŒåŒ…å«å…¨éƒ¨ä¸‰ä¸ªæ ‡ç­¾ï¼Œæ¯ä¸ªæ ‡ç­¾éƒ½å¿…é¡»æœ‰å®é™…å†…å®¹ï¼š

<output>
<ground_truth>
[æå–æœ€æ–°å›å¤å†…å®¹ï¼Œå»é™¤ç­¾åä½†ä¿ç•™æ‰€æœ‰æŠ€æœ¯ç»†èŠ‚ï¼šäººåã€ç³»ç»Ÿåã€è®¢å•å·ã€é…ç½®å€¼ç­‰]
</ground_truth>
<history>
[æå–å†å²é‚®ä»¶å¯¹è¯ã€‚å¦‚æœæ‰¾ä¸åˆ°"å‘ä»¶äºº:"/"From:"åˆ†éš”ç¬¦ï¼Œåˆ™å¡«å†™"æ— å†å²å¯¹è¯"]
</history>
<topic>
[ä»é‚®ä»¶å†…å®¹ä¸­æå–æ ¸å¿ƒä¸»é¢˜ï¼Œ10-20å­—ï¼Œå¿…é¡»æè¿°å…·ä½“ä¸šåŠ¡åœºæ™¯ã€‚ç¦æ­¢ä½¿ç”¨"é‚®ä»¶å¤„ç†"ã€"é‚®ä»¶å›å¤"ç­‰æ³›åŒ–è¯]
</topic>
</output>

ã€ç¤ºä¾‹ã€‘
é‚®ä»¶ï¼šHi team, I've resolved the ARN issue. Testing can begin.

å‘ä»¶äºº: John <john@example.com>
ä¸»é¢˜: ARN Issue
Can you check the ARN mapping?

æ­£ç¡®è¾“å‡ºï¼š
<output>
<ground_truth>
Hi team, I've resolved the ARN issue. Testing can begin.
</ground_truth>
<history>
å‘ä»¶äºº: John <john@example.com>
ä¸»é¢˜: ARN Issue
Can you check the ARN mapping?
</history>
<topic>
ARNæ˜ å°„é—®é¢˜è§£å†³é€šçŸ¥
</topic>
</output>

ã€å¤„ç†æ­¥éª¤ã€‘
1. è¯†åˆ«"å‘ä»¶äºº:"/"From:"åˆ†éš”ç¬¦ä½ç½®
2. åˆ†éš”ç¬¦ä¹‹å‰ â†’ ground_truthï¼ˆå»é™¤ç­¾åï¼‰
3. åˆ†éš”ç¬¦ä¹‹å â†’ historyï¼ˆå»é™¤å†—ä½™å£°æ˜ï¼‰
4. ä»æ•´ä½“å†…å®¹æå–å…·ä½“çš„ä¸šåŠ¡ä¸»é¢˜ â†’ topic

ç°åœ¨å¼€å§‹å¤„ç†ä¸Šè¿°é‚®ä»¶ï¼Œå¿…é¡»è¾“å‡ºå®Œæ•´çš„ä¸‰ä¸ªXMLæ ‡ç­¾ï¼š"""
    
    try:
        response = llm_client.complete(processing_prompt)
        result_text = response.text.strip()
        
        logger.debug(f"LLMè¿”å›å†…å®¹ï¼ˆå‰1000å­—ç¬¦ï¼‰:\n{result_text[:1000]}")
        
        import re
        ground_truth_match = re.search(r'<ground_truth>(.*?)</ground_truth>', result_text, re.DOTALL)
        history_match = re.search(r'<history>(.*?)</history>', result_text, re.DOTALL)
        topic_match = re.search(r'<topic>(.*?)</topic>', result_text, re.DOTALL)
        
        missing_fields = []
        if not ground_truth_match: missing_fields.append("ground_truth")
        if not history_match: missing_fields.append("history")
        if not topic_match: missing_fields.append("topic")
        
        if missing_fields and retry_count == 0:
            logger.warning(f"  ç¼ºå°‘å¿…å¡«å­—æ®µ: {missing_fields}ï¼Œé‡è¯•ä¸€æ¬¡...")
            return process_conversation_with_llm(emails_data, llm_client, retry_count=1)
        
        if missing_fields:
            logger.error(f"LLMè¿”å›å†…å®¹:\n{result_text}")
            raise ValueError(f"LLMè¿”å›æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘å¿…å¡«å­—æ®µ: {missing_fields}")
        
        ground_truth = ground_truth_match.group(1).strip()
        history = history_match.group(1).strip()
        topic = topic_match.group(1).strip()
        
        if not ground_truth:
            raise ValueError("ground_truthä¸èƒ½ä¸ºç©º")
        
        if not history:
            logger.warning("  historyä¸ºç©ºï¼Œè¦æ±‚LLMå¡«å……...")
            if retry_count == 0:
                return process_conversation_with_llm(emails_data, llm_client, retry_count=1)
            history = "æ— å†å²å¯¹è¯"
        
        generic_topics = ["é‚®ä»¶å¤„ç†", "é‚®ä»¶å›å¤", "é‚®ä»¶", "å¤„ç†", "å›å¤"]
        if topic in generic_topics:
            logger.warning(f"  topic '{topic}' æ˜¯æ³›åŒ–è¯ï¼Œè¦æ±‚é‡æ–°ç”Ÿæˆ...")
            if retry_count == 0:
                return process_conversation_with_llm(emails_data, llm_client, retry_count=1)
        
        logger.info(f"  âœ“ æå–æˆåŠŸ")
        logger.info(f"  ä¸»é¢˜: {topic}")
        
        return {
            'ground_truth': ground_truth,
            'history': history,
            'topic': topic
        }
        
    except Exception as e:
        logger.error(f"âœ— LLMå¤„ç†å¤±è´¥: {str(e)}")
        if retry_count < 1:
            logger.info("  å°è¯•é‡è¯•ä¸€æ¬¡...")
            return process_conversation_with_llm(emails_data, llm_client, retry_count=1)
        raise


        raise


async def process_single_email(
    conv_data: dict,
    idx: int,
    total: int,
    llm_client,
    eval_agent,
    semaphore: asyncio.Semaphore
):
    """
    å¤„ç†å•ä¸ªé‚®ä»¶çš„å®Œæ•´æµç¨‹ï¼ˆå¹¶è¡Œç‰ˆæœ¬ï¼‰
    
    Args:
        conv_data: åŒ…å« email_id, conversation_id, content
        idx: å½“å‰ç´¢å¼•
        total: æ€»æ•°
        llm_client: LLMå®¢æˆ·ç«¯
        eval_agent: è¯„ä¼°Agent
        semaphore: å¹¶å‘æ§åˆ¶ä¿¡å·é‡
    
    Returns:
        dict: å¤„ç†ç»“æœ {'success': bool, 'strategies': list, 'error': str}
    """
    async with semaphore:  # æ§åˆ¶å¹¶å‘æ•°
        email_id = conv_data['email_id']
        conversation_id = conv_data['conversation_id']
        email_content = conv_data['content']
        
        logger.info(f"\n{'='*60}")
        logger.info(f"æ­£åœ¨å¤„ç†ä¼šè¯ {idx}/{total} (Email ID: {email_id})")
        logger.info(f"{'='*60}")
        
        try:
            # --- æ­¥éª¤ A: é¢„å¤„ç† ---
            processed = process_conversation_with_llm([email_content], llm_client)
            
            topic = processed['topic']
            history = processed['history']
            ground_truth_raw = processed['ground_truth']
            
            logger.info(f"  [{idx}] ä¸»é¢˜: {topic}")
            
            # è°ƒç”¨workflow APIï¼ˆä½¿ç”¨é…ç½®çš„é‚®ç®±æˆ–é»˜è®¤å€¼ï¼‰
            # email_account = conv_data.get('email_account', DEFAULT_STAFF_EMAIL)
            # workflow_result = await call_workflow_extract_api(topic, email_account)
            workflow_result = {}  # æš‚æ—¶ä¸ä½¿ç”¨ workflowï¼Œè®¾ç½®ä¸ºç©ºå­—å…¸
            
            # æ„é€ question
            specific_question = f"{topic}éœ€è¦è”ç³»å“ªäº›äººï¼Ÿéœ€è¦æ£€æŸ¥å“ªäº›ç³»ç»Ÿï¼Ÿéœ€è¦æ‰§è¡Œå“ªäº›æ“ä½œï¼Ÿ"
            
            # é¢„å¤„ç†ground_truth
            # ground_truth_processed = await preprocess_ground_truth_to_steps(
            #     ground_truth_raw, 
            #     llm_client
            # )
            ground_truth_processed = ground_truth_raw
            
            # æ„é€ å•ä¸ªæ ·æœ¬
            sample = Sample(
                question=specific_question,
                context=json.dumps({
                    # "workflow_result": workflow_result,
                    "history": history
                }, ensure_ascii=False),
                ground_truth=ground_truth_processed
            )
            
            # --- æ­¥éª¤ B: å•æ ·æœ¬å¾®è°ƒ (5è½®) ---
            logger.info(f"  [{idx}] >> å¼€å§‹è®­ç»ƒ 5 è½®...")
            
            # æ¯ä¸ªä»»åŠ¡ä½¿ç”¨ç‹¬ç«‹çš„Playbookï¼ˆé¿å…å¹¶å‘å†²çªï¼‰
            local_playbook = Playbook()
            generator = Generator(llm_client)
            reflector = Reflector(llm_client)
            curator = Curator(llm_client)
            task_env = EmailTaskEnvironment(eval_agent)
            
            adapter = OfflineAdapter(
                playbook=local_playbook,
                generator=generator,
                reflector=reflector,
                curator=curator
            )
            
            # è¿è¡Œè®­ç»ƒ
            results = adapter.run(
                samples=[sample],
                environment=task_env,
                epochs=5
            )
            
            # è·å–å¾—åˆ†æœ€é«˜çš„ç»“æœ
            best_result = max(results, key=lambda r: r.environment_result.metrics.get('score', 0))
            final_score = best_result.environment_result.metrics.get('score', 0)
            # logger.info(f"  [{idx}] >> è®­ç»ƒå®Œæˆï¼Œæœ€é«˜å¾—åˆ†: {final_score:.2f} (å…± {len(results)} è½®)")
            # æ‰“å°æ‰€æœ‰è½®æ¬¡å¾—åˆ†
            scores = []
            final_idx = -1
            epoch_idx = 0
            for result in results:
                scores.append(result.environment_result.metrics.get('score', 0))
                if final_idx == -1 and result.environment_result.metrics.get('score', 0) == final_score:
                    final_idx = epoch_idx
                epoch_idx+=1
            logger.info(f"   [{idx}] >> è®­ç»ƒå®Œæˆï¼Œæœ€é«˜å¾—åˆ†: {final_score:.2f} æ‰€æœ‰: {scores} æœ€é«˜åˆ†è½®æ¬¡: {final_idx}")
            
            # --- æ­¥éª¤ C: æå–ç­–ç•¥ ---
            new_bullets = []
            if local_playbook._bullets:
                logger.info(f"  [{idx}] >> æœ¬æ¬¡äº§ç”Ÿ {len(local_playbook._bullets)} æ¡ç­–ç•¥")
                for bullet in local_playbook._bullets.values():
                    bullet_dict = {
                        "id": bullet.id,
                        "section": bullet.section,
                        "content": bullet.content,
                        "helpful": bullet.helpful,
                        "harmful": bullet.harmful
                    }
                    new_bullets.append(bullet_dict)
            
            # --- æ­¥éª¤ D: ä¿å­˜åˆ°æ•°æ®åº“ ---
            record_data = {
                'email_id': email_id,
                'conversation_id': "ticket_" + str(conversation_id),
                'topic': topic,
                'mirix_data': workflow_result,
                'ground_truth': ground_truth_processed,
                'learned_strategies': new_bullets,
                'final_score': final_score
            }
            
            save_learning_record(record_data)
            logger.info(f"  [{idx}] âœ“ å¤„ç†å®Œæˆå¹¶å·²ä¿å­˜")
            
            return {
                'success': True,
                'strategies': new_bullets,
                'email_id': email_id,
                'score': final_score
            }
            
        except Exception as e:
            logger.error(f"  [{idx}] âœ— å¤„ç†å¤±è´¥: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                'success': False,
                'strategies': [],
                'email_id': email_id,
                'error': str(e)
            }


async def test_multi_turn_email_learning(conversations_list: list, max_concurrent: int = 3): 
    """
    å¹¶è¡Œå¤„ç†é‚®ä»¶å­¦ä¹ 
    
    Args:
        conversations_list: é‚®ä»¶ä¼šè¯åˆ—è¡¨
        max_concurrent: æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤3ï¼Œå¯æ ¹æ®APIé™åˆ¶è°ƒæ•´ï¼‰
    """
    logger.info("=" * 60)
    logger.info(f"å¼€å§‹ ACE å¹¶è¡Œé‚®ä»¶å­¦ä¹ ï¼ˆå®æ—¶å…¥åº“æ¨¡å¼ï¼Œå¹¶å‘æ•°={max_concurrent}ï¼‰")
    logger.info("=" * 60)
    
    # åˆå§‹åŒ–æ•°æ®åº“è¡¨
    init_learning_db()
    
    # éªŒè¯è¾“å…¥æ•°æ®
    if not conversations_list:
        raise ValueError("conversations_list ä¸èƒ½ä¸ºç©ºï¼Œè¯·æä¾›é‚®ä»¶ä¼šè¯åˆ—è¡¨")
    
    logger.info(f"æ”¶åˆ° {len(conversations_list)} ä¸ªé‚®ä»¶ä¼šè¯")
    
    # 1. æ£€æŸ¥ç¯å¢ƒé…ç½®
    logger.info("\n[1/5] æ£€æŸ¥ç¯å¢ƒé…ç½®...")
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        logger.error("æœªæ‰¾åˆ° OPENAI_API_KEYï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®")
        return
    logger.info("âœ“ API Key å·²é…ç½®")
    
    # 2. åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
    logger.info("\n[2/5] åˆå§‹åŒ– LLM å®¢æˆ·ç«¯...")
    llm_client = LiteLLMClient(
        model="gpt-4o",
        temperature=0.3,
        max_tokens=2048
    )
    logger.info("âœ“ LLM å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
    
    # ğŸ”§ Monkey patch ACEçš„JSONè§£æ
    import ace.roles
    original_safe_json_loads = ace.roles._safe_json_loads
    def patched_safe_json_loads(text: str):
        cleaned = text.strip()
        if cleaned.startswith("```json"):
            cleaned = cleaned[7:]
        if cleaned.startswith("```"):
            cleaned = cleaned[3:]
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]
        cleaned = cleaned.strip()
        return original_safe_json_loads(cleaned)
    
    ace.roles._safe_json_loads = patched_safe_json_loads
    logger.info("âœ“ å·²åº”ç”¨JSONè§£æè¡¥ä¸")
    
    # 3. åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
    logger.info("\n[3/5] åˆ›å»ºè®­ç»ƒç¯å¢ƒ...")
    eval_agent = ACEEvaluationAgent(llm_client=llm_client)
    logger.info("âœ“ è®­ç»ƒç¯å¢ƒåˆ›å»ºå®Œæˆ")
    
    # 4. å¹¶è¡Œå¤„ç†é‚®ä»¶ä¼šè¯
    logger.info(f"\n[4/5] å¼€å§‹å¹¶è¡Œå¤„ç† {len(conversations_list)} ä¸ªé‚®ä»¶ä¼šè¯ï¼ˆå¹¶å‘æ•°={max_concurrent}ï¼‰...")
    
    # åˆ›å»ºå¹¶å‘æ§åˆ¶ä¿¡å·é‡
    semaphore = asyncio.Semaphore(max_concurrent)
    
    # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
    tasks = [
        process_single_email(conv_data, idx, len(conversations_list), llm_client, eval_agent, semaphore)
        for idx, conv_data in enumerate(conversations_list, 1)
    ]
    
    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # 5. ç»Ÿè®¡ç»“æœå¹¶åˆå¹¶ç­–ç•¥
    success_count = 0
    fail_count = 0
    all_strategies = []
    
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            logger.error(f"ä»»åŠ¡ {i+1} å¼‚å¸¸: {result}")
            fail_count += 1
        elif result.get('success'):
            success_count += 1
            all_strategies.extend(result.get('strategies', []))
        else:
            fail_count += 1
    
    # 6. ç»“æŸå¤„ç†
    logger.info(f"\n{'='*60}")
    logger.info(f"æ‰€æœ‰ä¼šè¯å¤„ç†å®Œæˆ")
    logger.info(f"{'='*60}")
    logger.info(f"âœ“ æˆåŠŸ: {success_count}")
    logger.info(f"âœ— å¤±è´¥: {fail_count}")
    logger.info(f"âœ“ æ€»å…±äº§ç”Ÿç­–ç•¥: {len(all_strategies)} æ¡")
    
    # 7. ä¿å­˜æœ€ç»ˆåˆå¹¶çš„Playbookï¼ˆå¯é€‰ï¼‰
    if all_strategies:
        logger.info("\nä¿å­˜æœ€ç»ˆç­–ç•¥æ±‡æ€»...")
        final_playbook = Playbook()
        # æ³¨æ„ï¼šè¿™é‡Œåªæ˜¯æ±‡æ€»ï¼Œå®é™…çš„ç­–ç•¥å·²ç»å­˜åœ¨æ•°æ®åº“ä¸­äº†
        logger.info(f"âœ“ ç­–ç•¥å·²æ±‡æ€»ï¼ˆå®é™…ç­–ç•¥å·²ä¿å­˜åœ¨æ•°æ®åº“ä¸­ï¼‰")
    
    return {'success_count': success_count, 'fail_count': fail_count, 'total_strategies': len(all_strategies)}


# ä¿ç•™æ—§çš„ä¸²è¡Œç‰ˆæœ¬ä½œä¸ºå¤‡ç”¨ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
async def test_multi_turn_email_learning_serial(conversations_list: list): 
    """ä¸²è¡Œç‰ˆæœ¬ï¼ˆæ—§ç‰ˆæœ¬ï¼Œä¿ç•™ä½œä¸ºå¤‡ç”¨ï¼‰"""
    logger.info("=" * 60)
    logger.info("å¼€å§‹ ACE é€ä¸ªé‚®ä»¶å­¦ä¹ ï¼ˆä¸²è¡Œæ¨¡å¼ï¼‰")
    logger.info("=" * 60)
    
    # åˆå§‹åŒ–æ•°æ®åº“è¡¨
    init_learning_db()
    
    # éªŒè¯è¾“å…¥æ•°æ®
    if not conversations_list:
        raise ValueError("conversations_list ä¸èƒ½ä¸ºç©ºï¼Œè¯·æä¾›é‚®ä»¶ä¼šè¯åˆ—è¡¨")
    
    logger.info(f"æ”¶åˆ° {len(conversations_list)} ä¸ªé‚®ä»¶ä¼šè¯")
    
    # 1. æ£€æŸ¥ç¯å¢ƒé…ç½®
    logger.info("\n[1/5] æ£€æŸ¥ç¯å¢ƒé…ç½®...")
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        logger.error("æœªæ‰¾åˆ° OPENAI_API_KEYï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®")
        return
    logger.info("âœ“ API Key å·²é…ç½®")
    
    # 2. åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
    logger.info("\n[2/5] åˆå§‹åŒ– LLM å®¢æˆ·ç«¯...")
    llm_client = LiteLLMClient(
        model="gpt-4o",
        temperature=0.3,
        max_tokens=2048
    )
    logger.info("âœ“ LLM å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
    
    # ğŸ”§ Monkey patch ACEçš„JSONè§£æ
    import ace.roles
    original_safe_json_loads = ace.roles._safe_json_loads
    def patched_safe_json_loads(text: str):
        cleaned = text.strip()
        if cleaned.startswith("```json"):
            cleaned = cleaned[7:]
        if cleaned.startswith("```"):
            cleaned = cleaned[3:]
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]
        cleaned = cleaned.strip()
        return original_safe_json_loads(cleaned)
    
    ace.roles._safe_json_loads = patched_safe_json_loads
    logger.info("âœ“ å·²åº”ç”¨JSONè§£æè¡¥ä¸")
    
    # 3. åˆ›å»ºè¯„ä¼°ç¯å¢ƒå’Œå…¨å±€Playbook
    logger.info("\n[3/5] åˆ›å»ºè®­ç»ƒç¯å¢ƒ...")
    eval_agent = ACEEvaluationAgent(llm_client=llm_client)
    task_env = EmailTaskEnvironment(eval_agent)
    
    # å…¨å±€Playbookï¼Œç”¨äºç´¯ç§¯ç­–ç•¥
    global_playbook = Playbook()
    
    # åˆå§‹åŒ–ACEç»„ä»¶
    generator = Generator(llm_client)
    reflector = Reflector(llm_client)
    curator = Curator(llm_client)
    
    logger.info("âœ“ è®­ç»ƒç¯å¢ƒåˆ›å»ºå®Œæˆ")
    
    # 4. é€ä¸ªå¤„ç†é‚®ä»¶ä¼šè¯
    logger.info(f"\n[4/5] å¼€å§‹é€ä¸ªå¤„ç† {len(conversations_list)} ä¸ªé‚®ä»¶ä¼šè¯...")
    
    success_count = 0
    fail_count = 0
    
    for idx, conv_data in enumerate(conversations_list, 1):
        email_id = conv_data['email_id']
        conversation_id = conv_data['conversation_id']
        email_content = conv_data['content']
        
        logger.info(f"\n{'='*60}")
        logger.info(f"æ­£åœ¨å¤„ç†ä¼šè¯ {idx}/{len(conversations_list)} (Email ID: {email_id})")
        logger.info(f"{'='*60}")
        
        try:
            # --- æ­¥éª¤ A: é¢„å¤„ç† ---
            processed = process_conversation_with_llm([email_content], llm_client)
            
            topic = processed['topic']
            history = processed['history']
            ground_truth_raw = processed['ground_truth']
            
            logger.info(f"  ä¸»é¢˜: {topic}")
            
            # è°ƒç”¨workflow API
            workflow_result = await call_workflow_extract_api(topic, "shelia.sun@item.com")
            
            # æ„é€ question
            specific_question = f"{topic}éœ€è¦è”ç³»å“ªäº›äººï¼Ÿéœ€è¦æ£€æŸ¥å“ªäº›ç³»ç»Ÿï¼Ÿéœ€è¦æ‰§è¡Œå“ªäº›æ“ä½œï¼Ÿ"
            
            # é¢„å¤„ç†ground_truth
            ground_truth_processed = await preprocess_ground_truth_to_steps(
                ground_truth_raw, 
                llm_client
            )
            
            # æ„é€ å•ä¸ªæ ·æœ¬
            sample = Sample(
                question=specific_question,
                context=json.dumps({
                    "workflow_result": workflow_result,
                    "history": history
                }, ensure_ascii=False),
                ground_truth=ground_truth_processed
            )
            
            # --- æ­¥éª¤ B: å•æ ·æœ¬å¾®è°ƒ (5è½®) ---
            logger.info(f"  >> å¼€å§‹é’ˆå¯¹è¯¥æ ·æœ¬è®­ç»ƒ 5 è½®...")
            
            initial_strategies = set(global_playbook._bullets.keys())
            
            adapter = OfflineAdapter(
                playbook=global_playbook,
                generator=generator,
                reflector=reflector,
                curator=curator
            )
            
            results = adapter.run(
                samples=[sample], 
                environment=task_env,
                epochs=5
            )
            
            # è·å–å¾—åˆ†æœ€é«˜çš„ç»“æœ
            best_result = max(results, key=lambda r: r.environment_result.metrics.get('score', 0))
            final_score = best_result.environment_result.metrics.get('score', 0)
            logger.info(f"  >> è®­ç»ƒå®Œæˆï¼Œæœ€é«˜å¾—åˆ†: {final_score:.2f} (å…± {len(results)} è½®)")
            
            # --- æ­¥éª¤ C: è®¡ç®—å¢é‡ç­–ç•¥å¹¶å…¥åº“ ---
            current_strategies = set(global_playbook._bullets.keys())
            new_strategy_ids = current_strategies - initial_strategies
            
            new_bullets = []
            if new_strategy_ids:
                logger.info(f"  >> æœ¬æ¬¡æ–°å¢ {len(new_strategy_ids)} æ¡ç­–ç•¥:")
                for bid in new_strategy_ids:
                    bullet = global_playbook._bullets[bid]
                    bullet_dict = {
                        "id": bullet.id,
                        "section": bullet.section,
                        "content": bullet.content,
                        "helpful": bullet.helpful,
                        "harmful": bullet.harmful
                    }
                    new_bullets.append(bullet_dict)
                    logger.info(f"     + [{bullet.section}] {bullet.content[:50]}...")
            else:
                logger.info(f"  >> æœ¬æ¬¡æœªäº§ç”Ÿæ–°ç­–ç•¥")
            
            # --- æ­¥éª¤ D: ä¿å­˜åˆ°æ•°æ®åº“ ---
            record_data = {
                'email_id': email_id,
                'conversation_id': conversation_id,
                'topic': topic,
                'workflow_data': workflow_result,
                'ground_truth': ground_truth_processed,
                'learned_strategies': new_bullets,
                'final_score': final_score
            }
            
            # save_learning_record(record_data)
            success_count += 1
            
        except Exception as e:
            logger.error(f"âœ— ä¼šè¯ {idx} å¤„ç†å¤±è´¥: {str(e)}")
            fail_count += 1
            continue
    
    # 5. ç»“æŸå¤„ç†
    logger.info(f"\n{'='*60}")
    logger.info(f"æ‰€æœ‰ä¼šè¯å¤„ç†å®Œæˆ")
    logger.info(f"{'='*60}")
    logger.info(f"âœ“ æˆåŠŸ: {success_count}")
    logger.info(f"âœ— å¤±è´¥: {fail_count}")
    logger.info(f"âœ“ æœ€ç»ˆç­–ç•¥æ€»æ•°: {len(global_playbook._bullets)}")
    
    # 6. ä¿å­˜æœ€ç»ˆå®Œæ•´çš„Playbook
    logger.info("\nä¿å­˜æœ€ç»ˆå®Œæ•´Playbook...")
    playbook_path = "trained_email_playbook_final.json"
    global_playbook.save_to_file(playbook_path)
    logger.info(f"âœ“ Playbookå·²ä¿å­˜åˆ°: {playbook_path}")
    
    return global_playbook


async def main_with_ticket_api(
    staff_id: str = None,
    staff_email: str = None,
    staff_name: str = None,
    staff_role: str = None,
    max_tickets: int = None,
    max_concurrent: int = 3,
    batch_size: int = None
):
    """
    ä» ticket API è¯»å– ticket ä¼šè¯å¹¶è¿›è¡ŒACEè®­ç»ƒï¼ˆæ‰¹æ¬¡å¤„ç†æ¨¡å¼ï¼ŒèŠ‚çœå†…å­˜ï¼‰
    
    Args:
        staff_id: å‘˜å·¥IDï¼Œé»˜è®¤ä½¿ç”¨é…ç½®çš„ DEFAULT_STAFF_ID
        staff_email: CSRé‚®ç®±ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®çš„ DEFAULT_STAFF_EMAIL
        staff_name: CSRå§“åï¼Œé»˜è®¤ä½¿ç”¨é…ç½®çš„ DEFAULT_STAFF_NAME
        staff_role: è§’è‰²ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®çš„ DEFAULT_STAFF_ROLE
        max_tickets: æœ€å¤§ticketæ•°é‡ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®çš„ MAX_TICKETS
        max_concurrent: æœ€å¤§å¹¶å‘æ•°
        batch_size: æ¯æ‰¹å¤„ç†çš„ticketæ•°é‡ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®çš„ BATCH_SIZE
    """
    print("\n" + "=" * 80)
    print("ACE æ‰¹é‡ ticket å­¦ä¹ è„šæœ¬ï¼ˆä» ticket API è¯»å–ï¼Œæ‰¹æ¬¡å¤„ç†æ¨¡å¼ï¼‰")
    print("=" * 80)
    
    # ä½¿ç”¨é»˜è®¤é…ç½®
    if staff_id is None:
        staff_id = DEFAULT_STAFF_ID
    if staff_email is None:
        staff_email = DEFAULT_STAFF_EMAIL
    if staff_name is None:
        staff_name = DEFAULT_STAFF_NAME
    if staff_role is None:
        staff_role = DEFAULT_STAFF_ROLE
    if max_tickets is None:
        max_tickets = MAX_TICKETS
    if batch_size is None:
        batch_size = BATCH_SIZE
    
    # 1. åˆå§‹åŒ–æ•°æ®åº“è¡¨
    init_learning_db()
    
    # 2. æ£€æŸ¥ç¯å¢ƒé…ç½®
    print("\n[æ­¥éª¤1] æ£€æŸ¥ç¯å¢ƒé…ç½®...")
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âœ— æœªæ‰¾åˆ° OPENAI_API_KEYï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®")
        return
    print("âœ“ API Key å·²é…ç½®")
    
    # 3. åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
    print("\n[æ­¥éª¤2] åˆå§‹åŒ– LLM å®¢æˆ·ç«¯...")
    llm_client = LiteLLMClient(
        model="gpt-4o",
        temperature=0.3,
        max_tokens=2048
    )
    print("âœ“ LLM å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
    
    # ğŸ”§ Monkey patch ACEçš„JSONè§£æ
    import ace.roles
    original_safe_json_loads = ace.roles._safe_json_loads
    def patched_safe_json_loads(text: str):
        cleaned = text.strip()
        if cleaned.startswith("```json"):
            cleaned = cleaned[7:]
        if cleaned.startswith("```"):
            cleaned = cleaned[3:]
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]
        cleaned = cleaned.strip()
        return original_safe_json_loads(cleaned)
    
    ace.roles._safe_json_loads = patched_safe_json_loads
    print("âœ“ å·²åº”ç”¨JSONè§£æè¡¥ä¸")
    
    # 4. åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
    print("\n[æ­¥éª¤3] åˆ›å»ºè®­ç»ƒç¯å¢ƒ...")
    eval_agent = EmailEvaluationAgent(llm_client=llm_client)
    print("âœ“ è®­ç»ƒç¯å¢ƒåˆ›å»ºå®Œæˆ")
    
    # 5. æŒ‰æ‰¹æ¬¡å¤„ç†
    print("\n[æ­¥éª¤4] å¼€å§‹æŒ‰æ‰¹æ¬¡å¤„ç† ticket ä¼šè¯...")
    print(f"  å‘˜å·¥ID: {staff_id}")
    print(f"  å‘˜å·¥é‚®ç®±: {staff_email}")
    print(f"  å‘˜å·¥å§“å: {staff_name}")
    print(f"  è§’è‰²: {staff_role}")
    print(f"  æœ€å¤§æ•°é‡: {max_tickets}")
    print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"  å¹¶å‘æ•°: {max_concurrent}")
    
    total_success = 0
    total_fail = 0
    total_strategies = 0
    batch_count = 0
    
    try:
        async for conversations_list in fetch_ticket_conversations_from_api_batch(
            staff_id=staff_id,
            staff_email=staff_email,
            staff_name=staff_name,
            max_tickets=max_tickets,
            batch_size=batch_size
        ):
            if not conversations_list:
                continue
            
            batch_count += 1
            print(f"\n{'='*60}")
            print(f"å¤„ç†ç¬¬ {batch_count} æ‰¹ï¼Œå…± {len(conversations_list)} ä¸ªä¼šè¯")
            print(f"{'='*60}")
            
            # è®­ç»ƒè¿™ä¸€æ‰¹
            try:
                result = await test_multi_turn_email_learning(conversations_list, max_concurrent=max_concurrent)
                batch_success = result.get('success_count', 0)
                batch_fail = result.get('fail_count', 0)
                batch_strategies = result.get('total_strategies', 0)
                
                total_success += batch_success
                total_fail += batch_fail
                total_strategies += batch_strategies
                
                print(f"\nç¬¬ {batch_count} æ‰¹å®Œæˆ:")
                print(f"  æˆåŠŸ: {batch_success}")
                print(f"  å¤±è´¥: {batch_fail}")
                print(f"  ç­–ç•¥: {batch_strategies} æ¡")
                
                # æ¸…ç†å†…å­˜ï¼ˆPythonä¼šè‡ªåŠ¨GCï¼Œä½†æ˜¾å¼åˆ é™¤å¯ä»¥æ›´å¿«é‡Šæ”¾ï¼‰
                del conversations_list
                
            except Exception as e:
                print(f"âœ— ç¬¬ {batch_count} æ‰¹è®­ç»ƒå¤±è´¥: {str(e)}")
                import traceback
                traceback.print_exc()
                total_fail += len(conversations_list)
                continue
        
        # 6. æ€»ç»“
        print(f"\n{'='*60}")
        print(f"æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆ")
        print(f"{'='*60}")
        print(f"âœ“ æ€»æˆåŠŸ: {total_success}")
        print(f"âœ— æ€»å¤±è´¥: {total_fail}")
        print(f"âœ“ æ€»ç­–ç•¥: {total_strategies} æ¡")
        print(f"âœ“ æ€»æ‰¹æ¬¡æ•°: {batch_count}")
        
    except Exception as e:
        print(f"âœ— ticket API å¤„ç†å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


async def main_with_database(user_id: int = 1952974833739087873, limit: int = 10, offset: int = 0):
    """
    ä»æ•°æ®åº“è¯»å–é‚®ä»¶ä¼šè¯å¹¶è¿›è¡ŒACEè®­ç»ƒï¼ˆä¸²è¡Œç‰ˆæœ¬ï¼‰
    
    Args:
        user_id: ç”¨æˆ·ID
        limit: æŸ¥è¯¢çš„ä¼šè¯æ•°é‡é™åˆ¶
        offset: æŸ¥è¯¢çš„åç§»é‡
    """
    print("\n" + "=" * 80)
    print("ACE æ‰¹é‡é‚®ä»¶å­¦ä¹ è„šæœ¬ï¼ˆä»æ•°æ®åº“è¯»å–ï¼Œä¸²è¡Œæ¨¡å¼ï¼‰")
    print("=" * 80)
    
    # 1. ä»æ•°æ®åº“æŸ¥è¯¢é‚®ä»¶ä¼šè¯
    print("\n[æ­¥éª¤1] ä»æ•°æ®åº“æŸ¥è¯¢é‚®ä»¶ä¼šè¯...")
    print(f"  ç”¨æˆ·ID: {user_id}")
    print(f"  ä¼šè¯æ•°é‡: {limit}")
    print(f"  åç§»é‡: {offset}")
    try:
        conversations_list = fetch_email_conversations_from_db(user_id=user_id, limit=limit, offset=offset)
        
        if not conversations_list:
            print("âœ— æœªæŸ¥è¯¢åˆ°ä»»ä½•é‚®ä»¶ä¼šè¯")
            return
        
        print(f"âœ“ æˆåŠŸæŸ¥è¯¢åˆ° {len(conversations_list)} ä¸ªä¼šè¯")
        
    except Exception as e:
        print(f"âœ— æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {str(e)}")
        return
    
    # 2. è°ƒç”¨ACEè®­ç»ƒï¼ˆä¸²è¡Œï¼‰
    print("\n[æ­¥éª¤2] å¼€å§‹ACEä¸²è¡Œè®­ç»ƒ...")
    try:
        playbook = await test_multi_turn_email_learning_serial(conversations_list)
        print(f"\nâœ“ è®­ç»ƒå®Œæˆï¼")
        print(f"  æœ€ç»ˆç­–ç•¥æ€»æ•°: {len(playbook._bullets)} æ¡")
        
    except Exception as e:
        print(f"âœ— è®­ç»ƒå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='ACE Ticket Learning Script')
    parser.add_argument('--source', choices=['ticket', 'database'], default='ticket',
                       help='æ•°æ®æºï¼šticket (ticket API) æˆ– database (æ•°æ®åº“)')
    parser.add_argument('--staff-id', type=str, default=None,
                       help='å‘˜å·¥IDï¼ˆticketæ¨¡å¼ï¼‰')
    parser.add_argument('--staff-email', type=str, default=None,
                       help='å‘˜å·¥é‚®ç®±ï¼ˆticketæ¨¡å¼ï¼‰')
    parser.add_argument('--staff-name', type=str, default=None,
                       help='å‘˜å·¥å§“åï¼ˆticketæ¨¡å¼ï¼‰')
    parser.add_argument('--staff-role', type=str, default=None,
                       help='å‘˜å·¥è§’è‰²ï¼ˆticketæ¨¡å¼ï¼‰')
    parser.add_argument('--max-tickets', type=int, default=None,
                       help='æœ€å¤§ticketæ•°é‡ï¼ˆticketæ¨¡å¼ï¼‰')
    parser.add_argument('--max-concurrent', type=int, default=3,
                       help='æœ€å¤§å¹¶å‘æ•°ï¼ˆticketæ¨¡å¼ï¼‰')
    parser.add_argument('--batch-size', type=int, default=None,
                       help='æ¯æ‰¹å¤„ç†çš„ticketæ•°é‡ï¼ˆticketæ¨¡å¼ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®çš„BATCH_SIZEï¼‰')
    parser.add_argument('--user-id', type=int, default=1952974833739087873,
                       help='ç”¨æˆ·IDï¼ˆdatabaseæ¨¡å¼ï¼‰')
    parser.add_argument('--limit', type=int, default=102,
                       help='æŸ¥è¯¢çš„ä¼šè¯æ•°é‡é™åˆ¶ï¼ˆdatabaseæ¨¡å¼ï¼‰')
    parser.add_argument('--offset', type=int, default=0,
                       help='æŸ¥è¯¢çš„åç§»é‡ï¼ˆdatabaseæ¨¡å¼ï¼‰')
    
    args = parser.parse_args()
    
    if args.source == 'ticket':
        print("=" * 80)
        print("å¼€å§‹ ACE ticket å­¦ä¹ è®­ç»ƒï¼ˆå¹¶è¡Œæ¨¡å¼ï¼‰")
        print("=" * 80)
        print(f"å‘˜å·¥ID: {args.staff_id or DEFAULT_STAFF_ID}")
        print(f"å‘˜å·¥é‚®ç®±: {args.staff_email or DEFAULT_STAFF_EMAIL}")
        print(f"å‘˜å·¥å§“å: {args.staff_name or DEFAULT_STAFF_NAME}")
        print(f"è§’è‰²: {args.staff_role or DEFAULT_STAFF_ROLE}")
        print(f"æœ€å¤§æ•°é‡: {args.max_tickets or MAX_TICKETS}")
        print(f"å¹¶å‘æ•°: {args.max_concurrent}")
        print("=" * 80)
        
        asyncio.run(main_with_ticket_api(
            staff_id=args.staff_id,
            staff_email=args.staff_email,
            staff_name=args.staff_name,
            staff_role=args.staff_role,
            max_tickets=args.max_tickets,
            max_concurrent=args.max_concurrent,
            batch_size=args.batch_size
        ))
    else:
        print("=" * 80)
        print("å¼€å§‹ ACE é‚®ä»¶å­¦ä¹ è®­ç»ƒï¼ˆä¸²è¡Œæ¨¡å¼ï¼‰")
        print("=" * 80)
        print(f"ç”¨æˆ·ID: {args.user_id}")
        print(f"ä¼šè¯æ•°é‡: {args.limit}")
        print(f"åç§»é‡: {args.offset}")
        print("=" * 80)
        
        asyncio.run(main_with_database(
            user_id=args.user_id,
            limit=args.limit,
            offset=args.offset
        ))
